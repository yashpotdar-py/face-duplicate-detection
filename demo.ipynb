{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a64498d",
   "metadata": {},
   "source": [
    "# Face Duplicate Detection Demo\n",
    "\n",
    "This notebook demonstrates the face duplicate detection system for bicycle videos using GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Import our detection system\n",
    "from src import DetectionConfig, DuplicateFaceDetector, FaceDetector\n",
    "from src.models import BoundingBox\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3483e2",
   "metadata": {},
   "source": [
    "## 1. System Setup and GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a7a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"System Information:\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        memory_gb = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        print(f\"  Memory: {memory_gb:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  GPU not available - will use CPU (slower performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2cf2a",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure detection parameters\n",
    "config = DetectionConfig(\n",
    "    min_confidence=0.6,      # Higher threshold for cleaner detections\n",
    "    match_threshold=0.5,     # Distance threshold for face matching\n",
    "    batch_size=8,            # Adjust based on GPU memory\n",
    "    log_level=\"INFO\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Detection Configuration:\")\n",
    "print(f\"  Min confidence: {config.min_confidence}\")\n",
    "print(f\"  Match threshold: {config.match_threshold}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235aefe7",
   "metadata": {},
   "source": [
    "## 3. Single Frame Face Detection Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize face detector\n",
    "face_detector = FaceDetector(config)\n",
    "\n",
    "# Create a sample frame (you can replace this with actual video frame)\n",
    "def create_sample_frame_with_faces():\n",
    "    \"\"\"Create a synthetic frame with multiple face-like regions for demo.\"\"\"\n",
    "    frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    frame[:] = (50, 100, 150)  # Background color\n",
    "    \n",
    "    # Add some face-like rectangular regions\n",
    "    face_positions = [(100, 150, 80, 100), (300, 200, 90, 110), (450, 100, 75, 95)]\n",
    "    \n",
    "    for i, (x, y, w, h) in enumerate(face_positions):\n",
    "        # Create face-like region\n",
    "        color = (200 + i * 20, 180 + i * 15, 160 + i * 10)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, -1)\n",
    "        \n",
    "        # Add \"eyes\"\n",
    "        eye_color = (50, 50, 50)\n",
    "        cv2.circle(frame, (x + w//3, y + h//3), 5, eye_color, -1)\n",
    "        cv2.circle(frame, (x + 2*w//3, y + h//3), 5, eye_color, -1)\n",
    "        \n",
    "        # Add \"mouth\"\n",
    "        cv2.ellipse(frame, (x + w//2, y + 2*h//3), (w//4, h//8), 0, 0, 180, eye_color, 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "# Create and display sample frame\n",
    "sample_frame = create_sample_frame_with_faces()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Sample Frame with Synthetic Faces\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate face detection on the sample frame\n",
    "print(\"Running face detection...\")\n",
    "detections = face_detector.detect_faces(sample_frame)\n",
    "\n",
    "print(f\"Found {len(detections)} faces\")\n",
    "\n",
    "# Visualize detections\n",
    "frame_with_boxes = sample_frame.copy()\n",
    "\n",
    "for i, (bbox, confidence) in enumerate(detections):\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(\n",
    "        frame_with_boxes,\n",
    "        (bbox.x, bbox.y),\n",
    "        (bbox.x + bbox.width, bbox.y + bbox.height),\n",
    "        (0, 255, 0),  # Green box\n",
    "        2\n",
    "    )\n",
    "    \n",
    "    # Add confidence label\n",
    "    label = f\"Face {i+1}: {confidence:.2f}\"\n",
    "    cv2.putText(\n",
    "        frame_with_boxes,\n",
    "        label,\n",
    "        (bbox.x, bbox.y - 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.5,\n",
    "        (0, 255, 0),\n",
    "        1\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(frame_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"Face Detection Results ({len(detections)} faces found)\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170fcda",
   "metadata": {},
   "source": [
    "## 4. Face Embedding Extraction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474aec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for detected faces\n",
    "if detections:\n",
    "    print(\"Extracting face embeddings...\")\n",
    "    bboxes = [bbox for bbox, _ in detections]\n",
    "    embeddings = face_detector.extract_embeddings(sample_frame, bboxes)\n",
    "    \n",
    "    print(f\"Extracted {len(embeddings)} embeddings\")\n",
    "    \n",
    "    if embeddings:\n",
    "        print(f\"Embedding shape: {embeddings[0].shape}\")\n",
    "        print(f\"Embedding range: [{embeddings[0].min():.3f}, {embeddings[0].max():.3f}]\")\n",
    "        \n",
    "        # Visualize embedding distributions\n",
    "        fig, axes = plt.subplots(1, min(3, len(embeddings)), figsize=(15, 4))\n",
    "        if len(embeddings) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, embedding in enumerate(embeddings[:3]):\n",
    "            if i < len(axes):\n",
    "                axes[i].hist(embedding, bins=50, alpha=0.7)\n",
    "                axes[i].set_title(f\"Face {i+1} Embedding Distribution\")\n",
    "                axes[i].set_xlabel(\"Embedding Value\")\n",
    "                axes[i].set_ylabel(\"Frequency\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No faces detected for embedding extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec356a",
   "metadata": {},
   "source": [
    "## 5. Simulated Video Processing Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate processing multiple video frames\n",
    "from src.models import FaceDetection\n",
    "\n",
    "def simulate_video_processing():\n",
    "    \"\"\"Simulate face detection across multiple video frames.\"\"\"\n",
    "    simulated_detections = []\n",
    "    \n",
    "    # Simulate 3 videos with known duplicate faces\n",
    "    videos = [\"bicycle_video_001.mp4\", \"bicycle_video_002.mp4\", \"bicycle_video_003.mp4\"]\n",
    "    \n",
    "    # Create base embeddings for 4 different people\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    person_embeddings = {\n",
    "        \"person_A\": np.random.randn(512),\n",
    "        \"person_B\": np.random.randn(512),\n",
    "        \"person_C\": np.random.randn(512),\n",
    "        \"person_D\": np.random.randn(512)\n",
    "    }\n",
    "    \n",
    "    detection_id = 0\n",
    "    \n",
    "    for video_idx, video_name in enumerate(videos):\n",
    "        # Each video has 2-4 detections\n",
    "        num_detections = np.random.randint(2, 5)\n",
    "        \n",
    "        for det_idx in range(num_detections):\n",
    "            # Randomly assign person (with some probability of duplicates)\n",
    "            if video_idx > 0 and np.random.random() < 0.4:  # 40% chance of duplicate\n",
    "                person = np.random.choice([\"person_A\", \"person_B\"])\n",
    "            else:\n",
    "                person = np.random.choice(list(person_embeddings.keys()))\n",
    "            \n",
    "            # Add small noise to embedding to simulate real detection variance\n",
    "            embedding = person_embeddings[person] + np.random.randn(512) * 0.1\n",
    "            \n",
    "            # Create detection\n",
    "            detection = FaceDetection(\n",
    "                face_id=f\"temp_face_{detection_id}\",\n",
    "                video_filename=video_name,\n",
    "                timestamp=f\"00:00:{det_idx*5:02d}.000\",\n",
    "                bounding_box=BoundingBox(\n",
    "                    x=100 + np.random.randint(-50, 50),\n",
    "                    y=150 + np.random.randint(-30, 30),\n",
    "                    width=80 + np.random.randint(-10, 20),\n",
    "                    height=100 + np.random.randint(-15, 15)\n",
    "                ),\n",
    "                confidence=0.7 + np.random.random() * 0.3,\n",
    "                embedding=embedding.tolist()\n",
    "            )\n",
    "            \n",
    "            simulated_detections.append(detection)\n",
    "            detection_id += 1\n",
    "    \n",
    "    return simulated_detections\n",
    "\n",
    "# Generate simulated detections\n",
    "print(\"Generating simulated video detections...\")\n",
    "simulated_detections = simulate_video_processing()\n",
    "\n",
    "print(f\"Generated {len(simulated_detections)} face detections across 3 videos\")\n",
    "\n",
    "# Display detection summary\n",
    "detection_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Video': det.video_filename,\n",
    "        'Timestamp': det.timestamp,\n",
    "        'Confidence': f\"{det.confidence:.3f}\",\n",
    "        'Bbox': f\"({det.bounding_box.x}, {det.bounding_box.y}, {det.bounding_box.width}, {det.bounding_box.height})\"\n",
    "    }\n",
    "    for det in simulated_detections\n",
    "])\n",
    "\n",
    "print(\"\\nDetection Summary:\")\n",
    "print(detection_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10791974",
   "metadata": {},
   "source": [
    "## 6. Face Clustering Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate face clustering to find duplicates\n",
    "from src.face_clusterer import FaceClusterer\n",
    "\n",
    "print(\"Running face clustering to identify duplicates...\")\n",
    "face_clusterer = FaceClusterer(config)\n",
    "\n",
    "# Cluster the simulated detections\n",
    "clustered_detections = face_clusterer.find_duplicate_faces(simulated_detections)\n",
    "\n",
    "print(f\"Clustering completed. Found {len(set(d.face_id for d in clustered_detections))} unique faces\")\n",
    "\n",
    "# Analyze clustering results\n",
    "face_counts = {}\n",
    "for detection in clustered_detections:\n",
    "    face_counts[detection.face_id] = face_counts.get(detection.face_id, 0) + 1\n",
    "\n",
    "# Find duplicate groups\n",
    "duplicate_groups = {fid: count for fid, count in face_counts.items() if count > 1}\n",
    "\n",
    "print(f\"\\nDuplicate Analysis:\")\n",
    "print(f\"  Total detections: {len(clustered_detections)}\")\n",
    "print(f\"  Unique faces: {len(face_counts)}\")\n",
    "print(f\"  Duplicate groups: {len(duplicate_groups)}\")\n",
    "\n",
    "if duplicate_groups:\n",
    "    print(\"\\nDuplicate Groups:\")\n",
    "    for face_id, count in sorted(duplicate_groups.items(), key=lambda x: x[1], reverse=True):\n",
    "        videos = set(d.video_filename for d in clustered_detections if d.face_id == face_id)\n",
    "        print(f\"  {face_id}: {count} detections across {len(videos)} videos\")\n",
    "        print(f\"    Videos: {', '.join(videos)}\")\n",
    "\n",
    "# Visualize clustering results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot 1: Face count distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "counts = list(face_counts.values())\n",
    "plt.hist(counts, bins=range(1, max(counts) + 2), alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Number of Detections per Face')\n",
    "plt.ylabel('Number of Faces')\n",
    "plt.title('Face Detection Frequency Distribution')\n",
    "plt.xticks(range(1, max(counts) + 1))\n",
    "\n",
    "# Plot 2: Video distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "video_counts = {}\n",
    "for detection in clustered_detections:\n",
    "    video_counts[detection.video_filename] = video_counts.get(detection.video_filename, 0) + 1\n",
    "\n",
    "videos = list(video_counts.keys())\n",
    "counts = list(video_counts.values())\n",
    "plt.bar(range(len(videos)), counts, alpha=0.7)\n",
    "plt.xlabel('Video')\n",
    "plt.ylabel('Number of Face Detections')\n",
    "plt.title('Face Detections per Video')\n",
    "plt.xticks(range(len(videos)), [v.split('_')[-1].split('.')[0] for v in videos])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30af48",
   "metadata": {},
   "source": [
    "## 7. Generate and Analyze Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a comprehensive report\n",
    "detector = DuplicateFaceDetector(config)\n",
    "processing_time = 45.6  # Simulated processing time\n",
    "\n",
    "report = detector.generate_report(clustered_detections, processing_time)\n",
    "\n",
    "print(\"Detection Report Summary:\")\n",
    "print(f\"  Total faces detected: {report.total_faces}\")\n",
    "print(f\"  Unique faces: {report.unique_faces}\")\n",
    "print(f\"  Duplicate groups: {report.duplicate_groups}\")\n",
    "print(f\"  Processing time: {report.processing_time:.1f} seconds\")\n",
    "print(f\"  Detection rate: {report.total_faces/report.processing_time:.1f} faces/second\")\n",
    "\n",
    "# Save report to file\n",
    "output_path = Path(\"demo_results.json\")\n",
    "report.to_json(output_path)\n",
    "print(f\"\\nReport saved to: {output_path}\")\n",
    "\n",
    "# Display sample detections\n",
    "print(\"\\nSample Detection Entries:\")\n",
    "sample_detections = report.detections[:3]\n",
    "for i, detection in enumerate(sample_detections):\n",
    "    print(f\"\\nDetection {i+1}:\")\n",
    "    print(f\"  Face ID: {detection.face_id}\")\n",
    "    print(f\"  Video: {detection.video_filename}\")\n",
    "    print(f\"  Timestamp: {detection.timestamp}\")\n",
    "    print(f\"  Confidence: {detection.confidence:.3f}\")\n",
    "    print(f\"  Bounding Box: [{detection.bounding_box.x}, {detection.bounding_box.y}, {detection.bounding_box.width}, {detection.bounding_box.height}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f76f95",
   "metadata": {},
   "source": [
    "## 8. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a00700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance characteristics\n",
    "print(\"Performance Analysis:\")\n",
    "\n",
    "# Simulate performance for different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "simulated_fps = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    # Simulate FPS based on batch size (realistic GPU scaling)\n",
    "    if config.device == \"cuda\":\n",
    "        base_fps = 45\n",
    "        efficiency = min(1.0, batch_size / 8)  # Efficiency peaks around batch_size 8\n",
    "        memory_penalty = max(0.7, 1 - (batch_size - 16) * 0.05) if batch_size > 16 else 1.0\n",
    "        fps = base_fps * efficiency * memory_penalty\n",
    "    else:\n",
    "        # CPU performance\n",
    "        fps = 8 + batch_size * 0.5  # Much slower, linear scaling\n",
    "    \n",
    "    simulated_fps.append(fps)\n",
    "\n",
    "# Plot performance curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batch_sizes, simulated_fps, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Processing Rate (detections/second)')\n",
    "plt.title(f'Performance vs Batch Size ({config.device.upper()})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(batch_sizes)\n",
    "\n",
    "# Highlight current configuration\n",
    "current_idx = batch_sizes.index(config.batch_size)\n",
    "plt.plot(config.batch_size, simulated_fps[current_idx], 'ro', markersize=12, \n",
    "         label=f'Current Config (batch_size={config.batch_size})')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Memory usage estimation\n",
    "print(f\"\\nEstimated Memory Usage (1080p frames):\")\n",
    "frame_size_mb = (1920 * 1080 * 3) / (1024**2)  # RGB frame\n",
    "batch_memory_mb = frame_size_mb * config.batch_size\n",
    "model_memory_mb = 800  # Estimated model memory\n",
    "total_memory_mb = batch_memory_mb + model_memory_mb\n",
    "\n",
    "print(f\"  Frame size: {frame_size_mb:.1f} MB\")\n",
    "print(f\"  Batch memory: {batch_memory_mb:.1f} MB\")\n",
    "print(f\"  Model memory: {model_memory_mb} MB\")\n",
    "print(f\"  Total GPU memory: {total_memory_mb:.1f} MB ({total_memory_mb/1024:.1f} GB)\")\n",
    "\n",
    "if config.device == \"cuda\" and torch.cuda.is_available():\n",
    "    available_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    usage_percent = (total_memory_mb/1024) / available_memory_gb * 100\n",
    "    print(f\"  GPU memory usage: {usage_percent:.1f}% of {available_memory_gb:.1f} GB\")\n",
    "    \n",
    "    if usage_percent > 80:\n",
    "        print(\"  ‚ö†Ô∏è  High memory usage - consider reducing batch size\")\n",
    "    elif usage_percent < 40:\n",
    "        print(\"  ‚úÖ Memory usage looks good - could increase batch size for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294704fd",
   "metadata": {},
   "source": [
    "## 9. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FACE DUPLICATE DETECTION DEMO SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüîß System Configuration:\")\n",
    "print(f\"   Device: {config.device.upper()}\")\n",
    "print(f\"   Batch Size: {config.batch_size}\")\n",
    "print(f\"   Min Confidence: {config.min_confidence}\")\n",
    "print(f\"   Match Threshold: {config.match_threshold}\")\n",
    "\n",
    "print(f\"\\nüìä Demo Results:\")\n",
    "print(f\"   Total Detections: {len(clustered_detections)}\")\n",
    "print(f\"   Unique Faces: {len(set(d.face_id for d in clustered_detections))}\")\n",
    "print(f\"   Duplicate Groups: {len([fid for fid, count in face_counts.items() if count > 1])}\")\n",
    "print(f\"   Estimated Processing Rate: {simulated_fps[current_idx]:.1f} detections/second\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "\n",
    "if config.device == \"cpu\":\n",
    "    print(\"   ‚Ä¢ Install CUDA-enabled PyTorch for 5-10x performance improvement\")\n",
    "    print(\"   ‚Ä¢ GPU acceleration is highly recommended for production use\")\n",
    "else:\n",
    "    if config.batch_size < 8:\n",
    "        print(\"   ‚Ä¢ Consider increasing batch size to 8-16 for better GPU utilization\")\n",
    "    print(\"   ‚Ä¢ Current GPU configuration looks good for production use\")\n",
    "\n",
    "print(f\"   ‚Ä¢ For bicycle videos, skip_frames=30 (1 fps) provides good accuracy/speed balance\")\n",
    "print(f\"   ‚Ä¢ Adjust match_threshold based on validation results (current: {config.match_threshold})\")\n",
    "print(f\"   ‚Ä¢ Monitor GPU memory usage during processing of large videos\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Place video files in data/Videos/ directory\")\n",
    "print(f\"   2. Run: python detect_duplicates.py detect --videos-dir data/Videos --output-file results.json\")\n",
    "print(f\"   3. Analyze results and tune parameters as needed\")\n",
    "print(f\"   4. Set up automated processing pipeline for production use\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
